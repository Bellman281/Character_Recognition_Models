{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import Normalize, ToTensor\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "import torch.nn.functional as F  \n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_backend():\n",
    "    global device\n",
    "    print(f\" Pytorch Version {torch.__version__}\")\n",
    "    print (f' MPS backend is bulit? {torch.backends.mps.is_built()}')\n",
    "    print( f' MPS backend is available {torch.backends.mps.is_available()}')\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    print(f' Device is set to {device}')\n",
    "    return \n",
    "\n",
    "\n",
    "# load data in\n",
    "\n",
    "def load_data(val_split=0.8):\n",
    "    \n",
    "    train_set = datasets.EMNIST(root=\"data\", split=\"balanced\", train=True, \n",
    "                                transform = transforms.Compose([ToTensor(),\n",
    "                               Normalize( (0.1307,), (0.3081,))]))\n",
    "                                                    \n",
    "    test_set = datasets.EMNIST(root=\"data\", split=\"balanced\", train=False, \n",
    "                               transform=transforms.Compose([\n",
    "                               ToTensor(),\n",
    "                               Normalize((0.1307,), (0.3081,))]))\n",
    "    \n",
    "    train_ = torch.utils.data.DataLoader(train_set, shuffle=True)\n",
    "\n",
    "    split_ = int(val_split*(len(train_)))  \n",
    "    valid_ = len(train_) - split_ \n",
    "\n",
    "    train_set, val_set = torch.utils.data.random_split(train_set, [split_, valid_]) \n",
    "\n",
    "    print(f' train size: {len(train_set)}, val size: {len(val_set)} , test size: {len(test_set)} ')\n",
    "    classes = test_set.classes\n",
    "    return train_set, val_set, test_set,classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "first_HL =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pytorch Version 1.12.0\n",
      " MPS backend is bulit? True\n",
      " MPS backend is available True\n",
      " Device is set to mps\n",
      " train size: 90240, val size: 22560 , test size: 18800 \n"
     ]
    }
   ],
   "source": [
    "check_backend()\n",
    "train_set, val_set, test_set,classes = load_data(val_split=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(160, first_HL) #changed from 16 to 8\n",
    "        self.fc1_1 = nn.Linear(160 + first_HL, first_HL) #added\n",
    "        self.fc1_2 = nn.Linear(160 + first_HL, first_HL) #added\n",
    "        self.fc1_3 = nn.Linear(160 + first_HL, first_HL) #added\n",
    "        self.fc1_4 = nn.Linear(160 + first_HL, first_HL) #added\n",
    "        self.fc1_5 = nn.Linear(160 + first_HL, first_HL) #added\n",
    "        self.fc2 = nn.Linear(first_HL*6, 47) # changed first_HL from second_HL\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x1 = x[:, 0:160]\n",
    "        \n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        x2= torch.cat([ x[:,160:320], x1], dim=1)\n",
    "        x2 = F.relu(self.fc1_1(x2))\n",
    "        x3= torch.cat([ x[:,0:160], x2], dim=1)\n",
    "        x3 = F.relu(self.fc1_2(x3))\n",
    "        x4= torch.cat([ x[:,160:320], x3], dim=1)\n",
    "        x4 = F.relu(self.fc1_3(x4))\n",
    "        x5= torch.cat([ x[:,0:160], x4], dim=1)\n",
    "        x5 = F.relu(self.fc1_4(x5))\n",
    "        x6= torch.cat([ x[:,160:320], x5], dim=1)\n",
    "        x6 = F.relu(self.fc1_5(x6))\n",
    "\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        x = torch.cat([x, x6], dim=1)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "    \n",
    "        return F.log_softmax(x, dim = 1)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate, Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_m(model, optimizer, criterion):\n",
    "  \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #3inputs, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, prediction = torch.max(outputs.data, 1)  \n",
    "        total += labels.size(0)\n",
    "        correct += (prediction == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    return model, train_loss, train_acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_m(model, criterion):\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size_train, shuffle=True)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (prediction == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_m(model):\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_explore(epochs = 2):\n",
    " \n",
    "    model = Net()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "    #summary(model, (1, 28, 28))\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    time_total = 0\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        time_start = time.time()\n",
    "        model, train_loss, train_acc = train_m(model, optimizer, criterion)\n",
    "        val_loss, val_acc = validate_m(model, criterion)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        time_duration = round(time_end - time_start, 2)\n",
    "        time_total += time_duration\n",
    "        \n",
    "      \n",
    "        print(f'Epoch {epoch},    train acc: {train_acc:6.2f},train loss: {train_loss:7.4f},\\t val acc: {val_acc:6.2f}, val loss: {val_loss:7.4f},  \\t time: {time_duration}s')\n",
    "    test_acc = test_m(model)\n",
    "\n",
    "    results = OrderedDict()\n",
    "    results['train_losses'] = [round(x, 4) for x in train_losses]\n",
    "    results['val_losses'] = [round(x, 4) for x in val_losses]\n",
    "    results['train_accs'] = [round(x, 2) for x in train_accs]\n",
    "    results['val_accs'] = [round(x, 2) for x in val_accs]\n",
    "    results['train_acc'] = round(train_acc, 2)\n",
    "    results['val_acc'] = round(val_acc, 2)\n",
    "    results['test_acc'] = round(test_acc, 2)\n",
    "    results['time_total'] = round(time_total, 2)\n",
    "    \n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0,    train acc:   9.43,train loss:  3.5623,\t val acc:  37.11, val loss:  2.3210,  \t time: 29.14s\n",
      "Epoch 1,    train acc:  40.25,train loss:  2.1936,\t val acc:  54.40, val loss:  1.6300,  \t time: 28.78s\n",
      "Epoch 2,    train acc:  48.26,train loss:  1.8655,\t val acc:  58.77, val loss:  1.4773,  \t time: 29.17s\n",
      "Epoch 3,    train acc:  51.15,train loss:  1.7545,\t val acc:  58.86, val loss:  1.4028,  \t time: 29.38s\n",
      "Epoch 4,    train acc:  50.89,train loss:  1.7349,\t val acc:  56.15, val loss:  1.4840,  \t time: 29.71s\n",
      "Epoch 5,    train acc:  50.18,train loss:  1.7496,\t val acc:  62.29, val loss:  1.2663,  \t time: 29.59s\n",
      "Epoch 6,    train acc:  54.25,train loss:  1.5974,\t val acc:  64.29, val loss:  1.1927,  \t time: 30.22s\n",
      "Epoch 7,    train acc:  54.02,train loss:  1.5945,\t val acc:  63.28, val loss:  1.1914,  \t time: 29.55s\n",
      "Epoch 8,    train acc:  52.65,train loss:  1.6258,\t val acc:  65.62, val loss:  1.1082,  \t time: 28.95s\n",
      "Epoch 9,    train acc:  54.13,train loss:  1.5619,\t val acc:  65.76, val loss:  1.1368,  \t time: 29.05s\n",
      "Epoch 10,    train acc:  56.26,train loss:  1.4932,\t val acc:  66.93, val loss:  1.0542,  \t time: 28.83s\n",
      "Epoch 11,    train acc:  56.68,train loss:  1.4660,\t val acc:  67.58, val loss:  1.0525,  \t time: 28.97s\n",
      "Epoch 12,    train acc:  54.83,train loss:  1.5327,\t val acc:  67.05, val loss:  1.0705,  \t time: 29.48s\n",
      "Epoch 13,    train acc:  55.11,train loss:  1.5257,\t val acc:  66.76, val loss:  1.0775,  \t time: 29.3s\n",
      "Epoch 14,    train acc:  56.14,train loss:  1.4851,\t val acc:  68.54, val loss:  1.0219,  \t time: 29.73s\n",
      "Epoch 15,    train acc:  55.22,train loss:  1.5226,\t val acc:  67.37, val loss:  1.0517,  \t time: 29.58s\n",
      "Epoch 16,    train acc:  54.32,train loss:  1.5500,\t val acc:  65.43, val loss:  1.1216,  \t time: 29.5s\n",
      "Epoch 17,    train acc:  55.01,train loss:  1.5413,\t val acc:  66.84, val loss:  1.0748,  \t time: 29.09s\n",
      "Epoch 18,    train acc:  55.29,train loss:  1.5184,\t val acc:  68.48, val loss:  1.0360,  \t time: 29.14s\n"
     ]
    }
   ],
   "source": [
    "results,model = model_explore(epochs= 100)\n",
    "\n",
    "print(f'test acc: {results[\"test_acc\"]}')\n",
    "print(f'total training time: {results[\"time_total\"]}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'EMNIST_SPINALNET_model_22JUL2022.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load('EMNIST_SPINALNET_model_22JUL2022.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "\n",
    "    x,y = test_set[i][0], test_set [i][1]\n",
    "    x = x.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        pred = loaded_model(x)\n",
    "        predicted , actual = classes[pred[0].argmax(0)],classes[y]\n",
    "        if  predicted != actual:\n",
    "            flag = 'Not correct'\n",
    "        else :\n",
    "             flag = ''\n",
    "        print(f'Imgae {i}, preicted : \"{predicted}\", actual:\"{actual}\" \\t {flag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this function you can see the image and its label form the dataset\n",
    "def to_char(num):\n",
    "    if num<10:\n",
    "        return str(num)\n",
    "    elif num < 36:\n",
    "        return chr(num+55)\n",
    "    else:\n",
    "        return chr(num+61)\n",
    "\n",
    "\n",
    "def show_example(data):\n",
    "    img, label = data\n",
    "    print(\"Label: (\"+to_char(label)+\")\")\n",
    "    plt.imshow(img[0], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(test_set[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
